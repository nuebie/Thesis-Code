{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import emoji\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ecacf",
   "metadata": {},
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filkeywords = ['\"patayin\" AND \"ang\" AND sarili\"', '\"gusto\" AND \"ko\" AND \"ng\" AND \"mamatay\"', '\"pagpapakamatay\"',\n",
    "               '\"hindi\" AND \"ipinanganak\"', '\"sana\" AND \"patay\" AND \"nalang\"', '\"hindi\" AND \"na\" AND \"magigising\"', \n",
    "               '\"ayoko\" AND \"ng\" AND \"mabuhay\"','\"tapusin\" AND \"ang\" AND \"buhay\"', '\"wala\" AND \"akong\" AND \"silbi\"']\n",
    "\n",
    "\n",
    "enkeywords = ['\"depressed\"', '\"depression\"', '\"want\" AND \"to\" AND \"die\"', '\"kill\" AND \"myself\"', '\"suicide\"', '\"suicidal\"', \n",
    "              '\"no\" AND \"reason\" AND \"to\" AND \"live\"', '\"hate\" AND \"my\" AND \"life\"', '\"end\" AND \"my\" AND \"life\"', \n",
    "              '\"cut\" AND \"myself\"']\n",
    "\n",
    "tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE FILIPINO KEYWORDS\n",
    "for i, k in enumerate(filkeywords):\n",
    "    print(\"SCRAPING KEYWORD: \",filkeywords[i])\n",
    "    query = filkeywords[i]+\" until:2022-11-23 since:2020-01-01\"\n",
    "    for filtweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        tweets.append([filtweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE ENGLISH KEYWORDS\n",
    "for x, y in enumerate(enkeywords):\n",
    "    print(\"SCRAPING KEYWORD: \",enkeywords[x])\n",
    "    query = enkeywords[x]+\" geocode:12.879721,121.774017,724km until:2022-11-23 since:2020-01-01\"\n",
    "    for entweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        tweets.append([entweet.content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bac15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets, columns = ['Tweet']) # CREATE DATAFRAME\n",
    "df.insert(1, \"language\", \"\") # INSERT A THE \"LANGUAGE\" COLUMN\n",
    "df.insert(2, \"Label\", \"\") # INSERT A THE \"LABEL\" COLUMN\n",
    "df.to_csv(r'D:\\user\\Thesis dataset\\suicide_ideation.csv', encoding='utf-8', index=False) #SAVE AS CSV FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2daff",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING (CLEANING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\PRESENTATION\\suicide_ideation.csv') # LOAD CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    temp = tweet.lower() # lower case the letters\n",
    "    temp = emoji.replace_emoji(temp, replace=\"\") #remove emoji\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp) # remove mentions\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp) # remove hashtags\n",
    "    temp = re.sub(r'http\\S+', '', temp) # remove links\n",
    "    temp = re.sub('[()!?]', ' ', temp) # remove punctations\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp) # remove non-alphanumeric characters\n",
    "    temp = ''.join([i for i in temp if not i.isdigit()]) #remove numbers\n",
    "    temp = \" \".join(temp.split()) # remove whitespace\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(clean_tweet) #APPLY THE FUNCTION TO THE \"TWEET\" COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"Tweet\"]) # drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ROWS WITH EMPTY/NULL TWEET CELLS\n",
    "df['Tweet'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['Tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ca1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\user\\Thesis dataset (updated)\\PRESENTATION\\cleaned_suicide_ideation.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf563657",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING (LANGUAGE IDENTIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae058645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\PRESENTATION\\cleaned_suicide_ideation_v2.csv') # LOAD CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ROWS THAT ARE NOT FILIPINO OR ENGLISH\n",
    "df = df.loc[(df[\"language\"] == \"fil\") | (df[\"language\"] == \"en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08eb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\user\\Thesis dataset (updated)\\PRESENTATION\\cleaned_suicide_ideation_v2.1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f9406",
   "metadata": {},
   "source": [
    "# DATASET BALANCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset\\suicide_ideation.csv') #LOAD CLEANED & LABELED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cac355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARATE TWEETS BASE ON PHRASES OR TERMS USED DURING SCRAPING\n",
    "mask = df['Tweet'].str.contains('patayin') & df['Tweet'].str.contains('ang') & df['Tweet'].str.contains('sarili')\n",
    "filtered_df1 = df[mask]\n",
    "df.drop(filtered_df1.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('sana') & df['Tweet'].str.contains('patay') & df['Tweet'].str.contains('nalang')\n",
    "filtered_df2 = df[mask]\n",
    "df.drop(filtered_df2.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('gusto') & df['Tweet'].str.contains('ko') & df['Tweet'].str.contains('ng') & df['Tweet'].str.contains('mamatay')\n",
    "filtered_df3 = df[mask]\n",
    "df.drop(filtered_df3.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('pagpapakamatay')\n",
    "filtered_df4 = df[mask]\n",
    "df.drop(filtered_df4.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('hindi') & df['Tweet'].str.contains('ipinanganak')\n",
    "filtered_df5 = df[mask]\n",
    "df.drop(filtered_df5.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('hindi') & df['Tweet'].str.contains('na') & df['Tweet'].str.contains('magigising')\n",
    "filtered_df6 = df[mask]\n",
    "df.drop(filtered_df6.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('ayoko') & df['Tweet'].str.contains('ng') & df['Tweet'].str.contains('mabuhay')\n",
    "filtered_df7 = df[mask]\n",
    "df.drop(filtered_df7.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('tapusin') & df['Tweet'].str.contains('ang') & df['Tweet'].str.contains('buhay')\n",
    "filtered_df8 = df[mask]\n",
    "df.drop(filtered_df8.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('wala') & df['Tweet'].str.contains('akong') & df['Tweet'].str.contains('silbi')\n",
    "filtered_df9 = df[mask]\n",
    "df.drop(filtered_df9.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('handa') & df['Tweet'].str.contains('na') & df['Tweet'].str.contains('akong') & df['Tweet'].str.contains('mamatay')\n",
    "filtered_df10 = df[mask]\n",
    "df.drop(filtered_df10.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('depressed')\n",
    "filtered_df11 = df[mask]\n",
    "df.drop(filtered_df11.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('suicide')\n",
    "filtered_df12 = df[mask]\n",
    "df.drop(filtered_df12.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('depression')\n",
    "filtered_df13 = df[mask]\n",
    "df.drop(filtered_df13.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('suicidal')\n",
    "filtered_df14 = df[mask]\n",
    "df.drop(filtered_df14.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('want') & df['Tweet'].str.contains('to') & df['Tweet'].str.contains('die')\n",
    "filtered_df15 = df[mask]\n",
    "df.drop(filtered_df15.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('kill') & df['Tweet'].str.contains('myself')\n",
    "filtered_df16 = df[mask]\n",
    "df.drop(filtered_df16.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('no') & df['Tweet'].str.contains('reason') & df['Tweet'].str.contains('to') & df['Tweet'].str.contains('live') \n",
    "filtered_df17 = df[mask]\n",
    "df.drop(filtered_df17.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('hate') & df['Tweet'].str.contains('my') & df['Tweet'].str.contains('life')\n",
    "filtered_df18 = df[mask]\n",
    "df.drop(filtered_df18.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('cut') & df['Tweet'].str.contains('myself')\n",
    "filtered_df19 = df[mask]\n",
    "df.drop(filtered_df19.index, inplace=True)\n",
    "\n",
    "mask = df['Tweet'].str.contains('end') & df['Tweet'].str.contains('my') & df['Tweet'].str.contains('life')\n",
    "filtered_df20 = df[mask]\n",
    "df.drop(filtered_df20.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_list = [filtered_df1, filtered_df2, filtered_df3, filtered_df4, filtered_df5, filtered_df6, filtered_df7,\n",
    "                   filtered_df8, filtered_df9, filtered_df10, filtered_df11, filtered_df12, filtered_df13, filtered_df14]\n",
    "filtered_df_list2 = [filtered_df15, filtered_df16, filtered_df17, filtered_df18, filtered_df19, filtered_df20]\n",
    "indexnum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b024ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtered_df in (filtered_df_list):\n",
    "    \n",
    "    # PERFORM DOWNSAMPLING\n",
    "    at_risk = filtered_df[filtered_df.Label=='A'] #MINORITY CLASS\n",
    "    not_at_risk = filtered_df[filtered_df.Label=='B'] #MAJORITY CLASS\n",
    "\n",
    "    not_at_risk_downsampled = resample(not_at_risk,\n",
    "                                n_samples=len(at_risk)) # match the minority class\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    downsampled = pd.concat([at_risk, not_at_risk_downsampled])\n",
    "    \n",
    "    if indexnum == 0:\n",
    "        balanced_df = downsampled\n",
    "    else:\n",
    "        balanced_df = balanced_df.append(downsampled, ignore_index=True)\n",
    "        \n",
    "    indexnum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPEND THE PHRASES AND TERMS THAT CONTAIN LESS THAN 100 TWEETS\n",
    "for filtered_df in (filtered_df_list2):\n",
    "    balanced_df = balanced_df.append(filtered_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe47081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPRESENT THE LABELS AS NUMBERS\n",
    "# Define a mapping function\n",
    "def map_class(x):\n",
    "    if x == 'A':\n",
    "        return 1\n",
    "    elif x == 'B':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].apply(map_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab859c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHUFFLE THE ROWS OF THE DATASET\n",
    "balanced_df = balanced_df.reindex(np.random.permutation(balanced_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(r'D:\\user\\Thesis dataset\\balanced_suicide_ideation.csv', encoding='utf-8', index=False) #SAVE AS CSV FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8bd11",
   "metadata": {},
   "source": [
    "# TOKENIZATION AND VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956910c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\BALANCED BY KEYWORD\\shuffled_balanced_full_suicide_ideation.csv') # LOAD THE BALANCED LABELED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157de8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "#GET THE FEATURES\n",
    "features = df[\"Tweet\"]\n",
    "#GET THE LABEL\n",
    "label = df[\"Label\"]\n",
    "\n",
    "X, y = features, label\n",
    "\n",
    "#SPLIT THE DATASET INTO TRAINING AND TEST DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=1, stratify=label)\n",
    "\n",
    "#TOKENIZE AND VECTORIZE THE TRAINING FEATURES\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "\n",
    "#VECTORIZE THE TEST FEATURES\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "#SAVE VOCABULARY\n",
    "pickle.dump(tfidf, open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tfidf_vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22050c09",
   "metadata": {},
   "source": [
    "# TRAINING AND TESTING OF MODELS WITH NO TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a201e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #MAKES USE OF THE BAGGING ENSEMBLE TECHNIQUE\n",
    "xgb = XGBClassifier() #MAKES USE OF THE BOOSTING ENSEMBLE TECHNIQUE\n",
    "\n",
    "\n",
    "#BASE MODELS FOR THE VOTING ENSEMBLE\n",
    "voting_svm = SVC(probability=True)\n",
    "voting_knn = KNeighborsClassifier()\n",
    "voting_nb = MultinomialNB()\n",
    "voting_rf = RandomForestClassifier()\n",
    "voting_xgb = XGBClassifier()\n",
    "\n",
    "# Define the voting ensemble model\n",
    "voting_model = VotingClassifier(estimators=[('voting_rf', voting_rf), \n",
    "                                            ('voting_xgb', voting_xgb), \n",
    "                                            ('voting_svm', voting_svm), \n",
    "                                            ('voting_knn', voting_knn), \n",
    "                                            ('voting_nb', voting_nb)], voting='soft')\n",
    "\n",
    "#BASE MODELS FOR THE STACKING ENSEMBLE\n",
    "stacking_svm = SVC(probability=True)\n",
    "stacking_knn = KNeighborsClassifier()\n",
    "stacking_nb = MultinomialNB()\n",
    "stacking_rf = RandomForestClassifier()\n",
    "stacking_xgb = XGBClassifier()\n",
    "\n",
    "# Define the stacking ensemble model\n",
    "stacking_model = StackingClassifier(estimators=[('stacking_knn', stacking_knn), \n",
    "                                                ('stacking_rf', stacking_rf), \n",
    "                                                ('stacking_xgb', stacking_xgb), \n",
    "                                                ('stacking_nb', stacking_nb), \n",
    "                                                ('stacking_svm', stacking_svm)], cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e412d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train_vec, y_train) #TRAIN RANDOM FOREST MODEL\n",
    "xgb.fit(X_train_vec, y_train) #TRAIN XGBOOST MODEL\n",
    "voting_model.fit(X_train_vec,y_train) #TRAIN VOTING MODEL\n",
    "stacking_model.fit(X_train_vec,y_train) #TRAIN STACKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1157992",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rf,xgb,voting_model,stacking_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    y_pred = model.predict(X_test_vec) #MAKE PREDICTIONS WITH THE TRAINED MODELS USING THE TESTING SET\n",
    "    \n",
    "    #PRINT OUT THE SCORE OF MODEL WITH THE DIFFERENT PERFORMANCE METRICS\n",
    "    print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"PRECISION: \", metrics.precision_score(y_test, y_pred))\n",
    "    print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"F1 SCORE OF THE MODEL: \", metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106aaff",
   "metadata": {},
   "source": [
    "# MODEL TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0373d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE SEARCH SPACE TO BE USED IN TUNING THE MODELS\n",
    "rf_search = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [5, 10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "xgb_search = {\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 0.5],\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1, 10],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1, 10]\n",
    "}\n",
    "\n",
    "voting_search = {\n",
    "    'rf__n_estimators': [500, 1000],\n",
    "    'rf__max_depth': [5, 10, 20],\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 7],\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'nb__alpha': [0.1, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "stacking_search = {\n",
    "    'rf__n_estimators': [500, 1000],\n",
    "    'rf__max_depth': [5, 10, 20],\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 7],\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'svm__gamma': ['scale', 'auto'],\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'nb__alpha': [0.1, 0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "voting_svm = SVC(probability=True)\n",
    "voting_knn = KNeighborsClassifier()\n",
    "voting_nb = MultinomialNB()\n",
    "voting_rf = RandomForestClassifier()\n",
    "voting_xgb = XGBClassifier()\n",
    "\n",
    "# Define the voting ensemble model\n",
    "voting_model = VotingClassifier(estimators=[('voting_rf', voting_rf), \n",
    "                                            ('voting_xgb', voting_xgb), \n",
    "                                            ('voting_svm', voting_svm), \n",
    "                                            ('voting_knn', voting_knn), \n",
    "                                            ('voting_nb', voting_nb)], voting='soft')\n",
    "\n",
    "stacking_svm = SVC(probability=True)\n",
    "stacking_knn = KNeighborsClassifier()\n",
    "stacking_nb = MultinomialNB()\n",
    "stacking_rf = RandomForestClassifier()\n",
    "stacking_xgb = XGBClassifier()\n",
    "\n",
    "# Define the stacking ensemble model\n",
    "stacking_model = StackingClassifier(estimators=[('stacking_knn', stacking_knn), \n",
    "                                                ('stacking_rf', stacking_rf), \n",
    "                                                ('stacking_xgb', stacking_xgb), \n",
    "                                                ('stacking_nb', stacking_nb), \n",
    "                                                ('stacking_svm', stacking_svm)], cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292110d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNE THE RANDOM FOREST\n",
    "gs = GridSearchCV(estimator=rf,param_grid=rf_search,cv=5)\n",
    "gs.fit(X_train_vec, y_train) # Fit the model grid search to the  training data\n",
    "print(\"Best hyperparameters:\", gs.best_params_)# Print the best hyperparameters found by the grid search\n",
    "best_model = gs.best_estimator_ # Get the best model from the grid search\n",
    "    \n",
    "y_pred = best_model.predict(X_test_vec) #MAKE PREDICTIONS WITH THE TUNED MODEL USING THE TESTING SET\n",
    "    \n",
    "#PRINT OUT THE SCORE OF MODEL WITH THE DIFFERENT PERFORMANCE METRICS\n",
    "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"PRECISION: \", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 SCORE OF THE MODEL: \", metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "pickle.dump(best_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tuned_rf.pk\", \"wb\")) #SAVE THE TUNED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgb,voting_model,stacking_model]\n",
    "search_spaces = [xgb_search,voting_search, stacking_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cf7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNE THE XGBOOST, VOTING, STACKING USING RANDOMIZEDSEARCHCV\n",
    "for model, search_space in zip(models, search_space):\n",
    "    \n",
    "    rs = RandomizedSearchCV(estimator=model,param_distributions=search_space,n_iter=150,cv=5)\n",
    "    rs.fit(X_train_vec, y_train) # Fit the model grid search to the  training data\n",
    "    print(\"Best hyperparameters:\", rs.best_params_)# Print the best hyperparameters found by the grid search\n",
    "    best_model = rs.best_estimator_ # Get the best model from the grid search\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_vec) #MAKE PREDICTIONS WITH THE TUNED MODEL USING THE TESTING SET\n",
    "    \n",
    "    modelname = type(model).__name__\n",
    "    print(modelname)\n",
    "    \n",
    "    #PRINT OUT THE SCORE OF MODEL WITH THE DIFFERENT PERFORMANCE METRICS\n",
    "    print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"PRECISION: \", metrics.precision_score(y_test, y_pred))\n",
    "    print(\"Recall: \", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"F1 SCORE OF THE MODEL: \", metrics.f1_score(y_test, y_pred))\n",
    "    \n",
    "    #SAVE THE TUNED MODELS\n",
    "    if modelname == \"XGBClassifier\":\n",
    "        pickle.dump(best_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tuned_xgb.pk\", \"wb\"))\n",
    "    if modelname == \"VotingClassifier\":\n",
    "        pickle.dump(best_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tuned_voting.pk\", \"wb\"))\n",
    "    if modelname == \"StackingClassifier\":\n",
    "        pickle.dump(best_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tuned_stacking.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8b7d3",
   "metadata": {},
   "source": [
    "# K-FOLD CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85001048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\BALANCED BY KEYWORD\\shuffled_balanced_full_suicide_ideation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "#GET THE FEATURES\n",
    "features = df[\"Tweet\"]\n",
    "#GET THE LABEL\n",
    "label = df[\"Label\"]\n",
    "\n",
    "X, y = features, label\n",
    "\n",
    "#TOKENIZE AND VECTORIZE THE TRAINING FEATURES\n",
    "X_vec = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752de60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #MAKES USE OF THE BAGGING ENSEMBLE TECHNIQUE\n",
    "xgb = XGBClassifier() #MAKES USE OF THE BOOSTING ENSEMBLE TECHNIQUE\n",
    "\n",
    "voting_svm = SVC(probability=True)\n",
    "voting_knn = KNeighborsClassifier()\n",
    "voting_nb = MultinomialNB()\n",
    "voting_rf = RandomForestClassifier()\n",
    "voting_xgb = XGBClassifier()\n",
    "\n",
    "# Define the voting ensemble model\n",
    "voting_model = VotingClassifier(estimators=[('voting_rf', voting_rf), \n",
    "                                            ('voting_xgb', voting_xgb), \n",
    "                                            ('voting_svm', voting_svm), \n",
    "                                            ('voting_knn', voting_knn), \n",
    "                                            ('voting_nb', voting_nb)], voting='soft')\n",
    "\n",
    "stacking_svm = SVC(probability=True)\n",
    "stacking_knn = KNeighborsClassifier()\n",
    "stacking_nb = MultinomialNB()\n",
    "stacking_rf = RandomForestClassifier()\n",
    "stacking_xgb = XGBClassifier()\n",
    "\n",
    "# Define the stacking ensemble model\n",
    "stacking_model = StackingClassifier(estimators=[('stacking_knn', stacking_knn), \n",
    "                                                ('stacking_rf', stacking_rf), \n",
    "                                                ('stacking_xgb', stacking_xgb), \n",
    "                                                ('stacking_nb', stacking_nb), \n",
    "                                                ('stacking_svm', stacking_svm)], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rf,xgb,voting_model,stacking_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold with the desired number of folds\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Performance metrics to use\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "for model in models:\n",
    "    results = cross_validate(model, X_vec, y, cv=skf, scoring=scoring)\n",
    "    \n",
    "    #PRINT MODEL NAME\n",
    "    print(type(model).__name__)\n",
    "\n",
    "    # Print the scores for each fold and the average scores\n",
    "    print(\"Accuracy:\", results['test_accuracy'])\n",
    "    print(\"Precision:\", results['test_precision'])\n",
    "    print(\"Recall:\", results['test_recall'])\n",
    "    print(\"F1-score:\", results['test_f1'])\n",
    "\n",
    "    print(f\"\\nAverage Accuracy: {results['test_accuracy'].mean()}\")\n",
    "    print(f\"Average Precision: {results['test_precision'].mean()}\")\n",
    "    print(f\"Average Recall: {results['test_recall'].mean()}\")\n",
    "    print(f\"Average F1-score: {results['test_f1'].mean()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920f9da",
   "metadata": {},
   "source": [
    "# TRAINING THE OTHER SETS OF MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\BALANCED BY KEYWORD\\shuffled_balanced_full_suicide_ideation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13582ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "#GET THE FEATURES\n",
    "features = df[\"Tweet\"]\n",
    "#GET THE LABEL\n",
    "label = df[\"Label\"]\n",
    "\n",
    "X, y = features, label\n",
    "\n",
    "#TOKENIZE AND VECTORIZE THE TRAINING FEATURES\n",
    "X_vec = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #MAKES USE OF THE BAGGING ENSEMBLE TECHNIQUE\n",
    "xgb = XGBClassifier() #MAKES USE OF THE BOOSTING ENSEMBLE TECHNIQUE\n",
    "\n",
    "voting_svm = SVC(probability=True)\n",
    "voting_knn = KNeighborsClassifier()\n",
    "voting_nb = MultinomialNB()\n",
    "voting_rf = RandomForestClassifier()\n",
    "voting_xgb = XGBClassifier()\n",
    "\n",
    "# Define the voting ensemble model\n",
    "voting_model = VotingClassifier(estimators=[('voting_rf', voting_rf), \n",
    "                                            ('voting_xgb', voting_xgb), \n",
    "                                            ('voting_svm', voting_svm), \n",
    "                                            ('voting_knn', voting_knn), \n",
    "                                            ('voting_nb', voting_nb)], voting='soft')\n",
    "\n",
    "stacking_svm = SVC(probability=True)\n",
    "stacking_knn = KNeighborsClassifier()\n",
    "stacking_nb = MultinomialNB()\n",
    "stacking_rf = RandomForestClassifier()\n",
    "stacking_xgb = XGBClassifier()\n",
    "\n",
    "# Define the stacking ensemble model\n",
    "stacking_model = StackingClassifier(estimators=[('stacking_knn', stacking_knn), \n",
    "                                                ('stacking_rf', stacking_rf), \n",
    "                                                ('stacking_xgb', stacking_xgb), \n",
    "                                                ('stacking_nb', stacking_nb), \n",
    "                                                ('stacking_svm', stacking_svm)], cv=5)\n",
    "\n",
    "rf.fit(X_vec, y) #TRAIN RANDOM FOREST MODEL\n",
    "xgb.fit(X_vec, y) #TRAIN XGBOOST MODEL\n",
    "voting_model.fit(X_vec,y) #TRAIN VOTING MODEL\n",
    "stacking_model.fit(X_vec,y) #TRAIN STACKING MODEL\n",
    "\n",
    "pickle.dump(tfidf, open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tfidf_vocab_17k.pkl\", \"wb\"))\n",
    "pickle.dump(rf,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\rf_demo_17k.pk\", \"wb\"))\n",
    "pickle.dump(xgb,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\xgb_demo_17k.pk\", \"wb\"))\n",
    "pickle.dump(voting_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\voting_demo_17k.pk\", \"wb\"))\n",
    "pickle.dump(stacking_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\stacking_demo_17k.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26a062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba651ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\user\\Thesis dataset (updated)\\FULL DATASET\\shuffled_cleaned_labeled_suicide_ideation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "\n",
    "#GET THE FEATURES\n",
    "features = df[\"Tweet\"]\n",
    "#GET THE LABEL\n",
    "label = df[\"Label\"]\n",
    "\n",
    "X, y = features, label\n",
    "\n",
    "#TOKENIZE AND VECTORIZE THE FEATURES\n",
    "X_vec = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af04c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #MAKES USE OF THE BAGGING ENSEMBLE TECHNIQUE\n",
    "xgb = XGBClassifier() #MAKES USE OF THE BOOSTING ENSEMBLE TECHNIQUE\n",
    "\n",
    "voting_svm = SVC(probability=True)\n",
    "voting_knn = KNeighborsClassifier()\n",
    "voting_nb = MultinomialNB()\n",
    "voting_rf = RandomForestClassifier()\n",
    "voting_xgb = XGBClassifier()\n",
    "\n",
    "# Define the voting ensemble model\n",
    "voting_model = VotingClassifier(estimators=[('voting_rf', voting_rf), \n",
    "                                            ('voting_xgb', voting_xgb), \n",
    "                                            ('voting_svm', voting_svm), \n",
    "                                            ('voting_knn', voting_knn), \n",
    "                                            ('voting_nb', voting_nb)], voting='soft')\n",
    "\n",
    "stacking_svm = SVC(probability=True)\n",
    "stacking_knn = KNeighborsClassifier()\n",
    "stacking_nb = MultinomialNB()\n",
    "stacking_rf = RandomForestClassifier()\n",
    "stacking_xgb = XGBClassifier()\n",
    "\n",
    "# Define the stacking ensemble model\n",
    "stacking_model = StackingClassifier(estimators=[('stacking_knn', stacking_knn), \n",
    "                                                ('stacking_rf', stacking_rf), \n",
    "                                                ('stacking_xgb', stacking_xgb), \n",
    "                                                ('stacking_nb', stacking_nb), \n",
    "                                                ('stacking_svm', stacking_svm)], cv=5)\n",
    "\n",
    "rf.fit(X_vec, y) #TRAIN RANDOM FOREST MODEL\n",
    "xgb.fit(X_vec, y) #TRAIN XGBOOST MODEL\n",
    "voting_model.fit(X_vec,y) #TRAIN VOTING MODEL\n",
    "stacking_model.fit(X_vec,y) #TRAIN STACKING MODEL\n",
    "\n",
    "pickle.dump(tfidf, open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\tfidf_vocab_60k.pkl\", \"wb\"))\n",
    "pickle.dump(rf,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\rf_demo_60k.pk\", \"wb\"))\n",
    "pickle.dump(xgb,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\xgb_demo_60k.pk\", \"wb\"))\n",
    "pickle.dump(voting_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\voting_demo_60k.pk\", \"wb\"))\n",
    "pickle.dump(stacking_model,open(r\"C:\\Users\\user\\Presentation\\Trained Models\\Demo Models\\stacking_demo_60k.pk\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
